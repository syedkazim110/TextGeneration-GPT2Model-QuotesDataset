# -*- coding: utf-8 -*-
"""GPT_TextGeneration.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sCUhhChEO9cf78qqMHWYiaRnXQbI4SwQ
"""

import os
os.environ["WANDB_MODE"] = "disabled"
os.environ["WANDB_DISABLED"] = "true"

!pip install transformers datasets evaluate accelerate

from datasets import load_dataset

dataset = load_dataset("Abirate/english_quotes")


print(dataset["train"][0])

from transformers import AutoTokenizer

# Load GPT-2 tokenizer
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# GPT-2 doesn't have a pad token, so we set it to EOS token
tokenizer.pad_token = tokenizer.eos_token

# Function to tokenize dataset
def tokenize_function(examples):
    return tokenizer(
        examples["quote"],
        truncation=True,
        padding="max_length",
        max_length=50  # limit to 50 tokens for faster training
    )

# Apply tokenization
tokenized_datasets = dataset.map(tokenize_function, batched=True)

# For language modeling, we set labels = input_ids
tokenized_datasets = tokenized_datasets.map(
    lambda examples: {"labels": examples["input_ids"]},
    batched=True
)

from transformers import AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained("gpt2")

# Set pad token id same as EOS
model.config.pad_token_id = model.config.eos_token_id

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./gpt2-quotes",
    eval_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=2,
    weight_decay=0.01,
    save_strategy="epoch",
    logging_dir="./logs",
    logging_steps=50,
    push_to_hub=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"].shuffle(seed=42).select(range(2000)), # use small subset
    eval_dataset=tokenized_datasets["train"].shuffle(seed=42).select(range(500)),
)

trainer.train()

trainer.save_model("./gpt2-finetuned-quotes")
tokenizer.save_pretrained("./gpt2-finetuned-quotes")

from transformers import pipeline

generator = pipeline("text-generation", model="gpt2-finetuned-quotes")

output = generator(
    "The purpose of life is",
    max_length=80,           # allow more room to finish
    temperature=0.8,         # add randomness
    top_k=50,                # avoid always picking top 1 token
    top_p=0.9,                # nucleus sampling for variety
    repetition_penalty=1.8,  # punish repeated tokens
    num_return_sequences=1
)

print(output[0]['generated_text'])

from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

model_path = "/content/gpt2-finetuned-quotes"  # path to your LoRA/finetuned weights
model = GPT2LMHeadModel.from_pretrained(model_path)
tokenizer = GPT2Tokenizer.from_pretrained(model_path)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

eval_text = """When I do good, I feel good. When I do bad, I feel bad. That's my religion"""
encodings = tokenizer(eval_text, return_tensors="pt").to(device)

import math

with torch.no_grad():
    outputs = model(**encodings, labels=encodings["input_ids"])
    loss = outputs.loss
    perplexity = math.exp(loss.item())

print(f"Loss: {loss.item():.4f}")
print(f"Perplexity: {perplexity:.4f}")